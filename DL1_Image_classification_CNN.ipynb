{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_classification_CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "mar28",
      "language": "python",
      "name": "mar28"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bv8L2UKaffbp",
        "outputId": "677c49fc-eb40-44d1-e14f-9570f58cfad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm\n",
        "import tarfile\n",
        "\n",
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
        "\n",
        "class DLProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "if not isfile('cifar-10-python.tar.gz'):\n",
        "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
        "        urlretrieve(\n",
        "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
        "            'cifar-10-python.tar.gz',\n",
        "            pbar.hook)\n",
        "\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CIFAR-10 Dataset: 171MB [00:09, 18.0MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_hG8kXGLffb8"
      },
      "source": [
        "## The following are some helper functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lftVmXGeffb-",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "def _load_label_names():\n",
        "    \"\"\"\n",
        "    Load the label names from file\n",
        "    \"\"\"\n",
        "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
        "    \"\"\"\n",
        "    Load a batch of the dataset\n",
        "    \"\"\"\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
        "    \"\"\"\n",
        "    Display Stats of the the dataset\n",
        "    \"\"\"\n",
        "    batch_ids = list(range(1, 6))\n",
        "\n",
        "    if batch_id not in batch_ids:\n",
        "        print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
        "        return None\n",
        "\n",
        "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
        "\n",
        "    if not (0 <= sample_id < len(features)):\n",
        "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
        "        return None\n",
        "\n",
        "    print('\\nStats of batch {}:'.format(batch_id))\n",
        "    print('Samples: {}'.format(len(features)))\n",
        "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n",
        "    print('First 20 Labels: {}'.format(labels[:20]))\n",
        "\n",
        "    sample_image = features[sample_id]\n",
        "    sample_label = labels[sample_id]\n",
        "    label_names = _load_label_names()\n",
        "\n",
        "    print('\\nExample of Image {}:'.format(sample_id))\n",
        "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
        "    print('Image - Shape: {}'.format(sample_image.shape))\n",
        "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(sample_image)\n",
        "\n",
        "\n",
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    \"\"\"\n",
        "    Preprocess data and save it to file\n",
        "    \"\"\"\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    \"\"\"\n",
        "    Preprocess Training and Validation Data\n",
        "    \"\"\"\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        validation_count = int(len(features) * 0.1)\n",
        "\n",
        "        # Prprocess and save a batch of training data\n",
        "        _preprocess_and_save(\n",
        "            normalize,\n",
        "            one_hot_encode,\n",
        "            features[:-validation_count],\n",
        "            labels[:-validation_count],\n",
        "            'preprocess_batch_' + str(batch_i) + '.p')\n",
        "\n",
        "        # Use a portion of training batch for validation\n",
        "        valid_features.extend(features[-validation_count:])\n",
        "        valid_labels.extend(labels[-validation_count:])\n",
        "\n",
        "    # Preprocess and Save all validation data\n",
        "    _preprocess_and_save(\n",
        "        normalize,\n",
        "        one_hot_encode,\n",
        "        np.array(valid_features),\n",
        "        np.array(valid_labels),\n",
        "        'preprocess_validation.p')\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # load the training data\n",
        "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "\n",
        "    # Preprocess and Save all training data\n",
        "    _preprocess_and_save(\n",
        "        normalize,\n",
        "        one_hot_encode,\n",
        "        np.array(test_features),\n",
        "        np.array(test_labels),\n",
        "        'preprocess_training.p')\n",
        "\n",
        "\n",
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "\n",
        "def load_preprocess_training_batch(batch_id, batch_size):\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
        "    features, labels = pickle.load(open(filename, mode='rb'))\n",
        "\n",
        "    # Return the training data in batches of size <batch_size> or less\n",
        "    return batch_features_labels(features, labels, batch_size)\n",
        "\n",
        "\n",
        "def display_image_predictions(features, labels, predictions):\n",
        "    n_classes = 10\n",
        "    label_names = _load_label_names()\n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(range(n_classes))\n",
        "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
        "\n",
        "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
        "\n",
        "    n_predictions = 3\n",
        "    margin = 0.05\n",
        "    ind = np.arange(n_predictions)\n",
        "    width = (1. - 2. * margin) / n_predictions\n",
        "\n",
        "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
        "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
        "        correct_name = label_names[label_id]\n",
        "\n",
        "        axies[image_i][0].imshow(feature*255)\n",
        "        axies[image_i][0].set_title(correct_name)\n",
        "        axies[image_i][0].set_axis_off()\n",
        "\n",
        "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
        "        axies[image_i][1].set_yticks(ind + margin)\n",
        "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
        "        axies[image_i][1].set_xticks([0, 0.5, 1.0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z3fO1CELffcK",
        "outputId": "fa7cdd51-047c-498f-c11e-01eb9ceb4153",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Explore the dataset\n",
        "batch_id = 3\n",
        "sample_id = 5\n",
        "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Stats of batch 3:\n",
            "Samples: 10000\n",
            "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
            "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
            "\n",
            "Example of Image 5:\n",
            "Image - Min Value: 9 Max Value: 255\n",
            "Image - Shape: (32, 32, 3)\n",
            "Label - Label Id: 2 Name: bird\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAHxCAYAAABwLPU6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMUodwAoRrkDQDHKHQCKUe4AUMz/AkWuNrnrQewfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 251,
              "height": 248
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "msav2B5_ffcX"
      },
      "source": [
        "## Implement Preprocess Functions\n",
        "### Normalize\n",
        "The `normalize` function is implemented to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P8__ssNjffcZ",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a list of sample image data in the range of 0 to 1\n",
        "    : x: List of image data.  The image shape is (32, 32, 3)\n",
        "    : return: Numpy array of normalize data\n",
        "    \"\"\"\n",
        "    x = np.array(x)\n",
        "    x_max = np.max(x)\n",
        "    x_min = np.min(x)\n",
        "    output = (x - x_min)/(x_max - x_min)\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0C4Lsg5ffcj"
      },
      "source": [
        "### One-hot encode\n",
        "Implement the `one_hot_encode` function. The input, `x`, are a list of labels.  The function returns the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  The map of encodings should be saved outside the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L1kJ_BVYffcp",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit(range(10))\n",
        "def one_hot_encode(x):\n",
        "    \"\"\"\n",
        "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
        "    : x: List of sample Labels\n",
        "    : return: Numpy array of one-hot encoded labels\n",
        "    \"\"\"\n",
        "    \n",
        "    return lb.transform(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYE94652ffcv"
      },
      "source": [
        "### Randomize Data\n",
        "Now the order of the samples are randomized.  It doesn't hurt to randomize it again, but it's not needed for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zaoMxAuxffc1"
      },
      "source": [
        "## Preprocess all the data and save it\n",
        "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_V7NMa9_ffc6",
        "colab": {}
      },
      "source": [
        "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNlxSb2JffdE",
        "colab": {}
      },
      "source": [
        "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ztv1A-0NffdT"
      },
      "source": [
        "## Build the network\n",
        "\n",
        "### Input\n",
        "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
        "* Implement `neural_net_image_input`\n",
        "* Implement `neural_net_label_input`\n",
        "* Implement `neural_net_keep_prob_input`\n",
        "\n",
        "These names will be used at the end of the project to load your saved model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cJjgIvpyffdW",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def neural_net_image_input(image_shape):\n",
        "    \"\"\"\n",
        "    Return a Tensor for a bach of image input\n",
        "    : image_shape: Shape of the images\n",
        "    : return: Tensor for image input.\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.placeholder(tf.float32, shape = (None, image_shape[0], image_shape[1], image_shape[2]), name = 'x')\n",
        "\n",
        "\n",
        "def neural_net_label_input(n_classes):\n",
        "    \"\"\"\n",
        "    Return a Tensor for a batch of label input\n",
        "    : n_classes: Number of classes\n",
        "    : return: Tensor for label input.\n",
        "    \"\"\"\n",
        "    return tf.placeholder(tf.float32, shape = (None, n_classes), name = 'y')\n",
        "\n",
        "\n",
        "def neural_net_keep_prob_input():\n",
        "    \"\"\"\n",
        "    Return a Tensor for keep probability\n",
        "    : return: Tensor for keep probability.\n",
        "    \"\"\"\n",
        "    return tf.placeholder(tf.float32, shape = (None), name = 'keep_prob')\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HYnol51iffdf"
      },
      "source": [
        "### Convolution and Max Pooling Layer\n",
        "Implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
        "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
        "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
        "* Add bias\n",
        "* Add a nonlinear activation to the convolution.\n",
        "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I10m2DCjffdh",
        "colab": {}
      },
      "source": [
        "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
        "    \"\"\"\n",
        "    Apply convolution then max pooling to x_tensor\n",
        "    :param x_tensor: TensorFlow Tensor\n",
        "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
        "    :param conv_strides: Stride 2-D Tuple for convolution\n",
        "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
        "    :param pool_strides: Stride 2-D Tuple for pool\n",
        "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
        "    \"\"\"\n",
        "    shape_of_x_tensor = x_tensor.get_shape().as_list()\n",
        "    F_W = tf.Variable(tf.truncated_normal([conv_ksize[0] ,conv_ksize[1] ,shape_of_x_tensor[-1] ,conv_num_outputs] \n",
        "                                          ,dtype=tf.float32, stddev=0.2))\n",
        "    F_b = tf.Variable(tf.zeros([conv_num_outputs]  ,dtype=tf.float32))\n",
        "    strides_conv = [1, conv_strides[0], conv_strides[1], 1]\n",
        "    padding = 'SAME'\n",
        "    output = tf.nn.conv2d(x_tensor, F_W, strides_conv, padding)\n",
        "    output = tf.nn.bias_add(output, F_b)\n",
        "    output = tf.nn.relu(output)\n",
        "    ksize_maxpool = [1, pool_ksize[0], pool_ksize[1], 1]\n",
        "    strides_maxpool = [1, pool_strides[0], pool_strides[1], 1]\n",
        "    output = tf.nn.max_pool(output, ksize_maxpool, strides_maxpool, padding)\n",
        "    \n",
        "    return output \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bOPLOICLffdq"
      },
      "source": [
        "### Flatten Layer\n",
        "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F350AmREffdv",
        "colab": {}
      },
      "source": [
        "def flatten(x_tensor):\n",
        "    \"\"\"\n",
        "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
        "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
        "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    shape = x_tensor.get_shape().as_list()\n",
        "    flat_dim = shape[1]*shape[2]*shape[3]\n",
        "    output = tf.reshape(x_tensor, [-1, flat_dim])\n",
        "    return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KFG7H-0offd7"
      },
      "source": [
        "### Fully-Connected Layer\n",
        "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a1X-6y5Tffd9",
        "colab": {}
      },
      "source": [
        "def fully_conn(x_tensor, num_outputs):\n",
        "    \"\"\"\n",
        "    Apply a fully connected layer to x_tensor using weight and bias\n",
        "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
        "    : num_outputs: The number of output that the new tensor should be.\n",
        "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
        "    \"\"\"\n",
        "    # TODO: Implement Function\n",
        "    shape = x_tensor.get_shape().as_list()\n",
        "    weights= tf.Variable(tf.truncated_normal([shape[1], num_outputs] ,dtype=tf.float32, stddev=0.2))\n",
        "    biases = tf.Variable(tf.zeros([num_outputs]  ,dtype=tf.float32))\n",
        "    out = tf.add(tf.matmul(x_tensor, weights), biases)\n",
        "    out = tf.nn.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LWyaDqWSffeC"
      },
      "source": [
        "### Output Layer\n",
        "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iI3qRtn4ffeD",
        "colab": {}
      },
      "source": [
        "def output(x_tensor, num_outputs):\n",
        "    \"\"\"\n",
        "    Apply a output layer to x_tensor using weight and bias\n",
        "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
        "    : num_outputs: The number of output that the new tensor should be.\n",
        "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
        "    \"\"\"\n",
        "    shape = x_tensor.get_shape().as_list()\n",
        "    weights= tf.Variable(tf.truncated_normal([shape[1], num_outputs] ,dtype=tf.float32, stddev=0.2))\n",
        "    biases = tf.Variable(tf.zeros([num_outputs]  ,dtype=tf.float32))\n",
        "    out = tf.add(tf.matmul(x_tensor, weights), biases)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNqWqVz6ffeS"
      },
      "source": [
        "### Create Convolutional Model\n",
        "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits. The above layers userd to create this model:\n",
        "\n",
        "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
        "* Apply a Flatten Layer\n",
        "* Apply 1, 2, or 3 Fully Connected Layers\n",
        "* Apply an Output Layer\n",
        "* Return the output\n",
        "* Apply dropout to one or more layers in the model using `keep_prob`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fs6MB-LRffeV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "c1c84be3-65f1-44e4-8e99-132580a3642a"
      },
      "source": [
        "def conv_net(x, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a convolutional neural network model\n",
        "    : x: Placeholder tensor that holds image data.\n",
        "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
        "    : return: Tensor that represents logits\n",
        "    \"\"\"\n",
        "\n",
        "    out = conv2d_maxpool(x, conv_num_outputs = 16, conv_ksize = (3,3), conv_strides = (1,1), pool_ksize=(2,2), pool_strides=(2,2))\n",
        "    out = conv2d_maxpool(out, conv_num_outputs = 32, conv_ksize = (3,3), conv_strides = (1,1), pool_ksize=(2,2), pool_strides=(2,2))\n",
        "    out = conv2d_maxpool(out, conv_num_outputs = 64, conv_ksize = (3,3), conv_strides = (1,1), pool_ksize=(2,2), pool_strides=(2,2))\n",
        "\n",
        "    # Function Definition from Above:\n",
        "    out = flatten(out)\n",
        "    \n",
        "    \n",
        "\n",
        "    out = fully_conn(out, num_outputs = 64)\n",
        "    out = tf.nn.dropout(out, keep_prob)\n",
        "    out = fully_conn(out, num_outputs = 32)\n",
        "    out = tf.nn.dropout(out, keep_prob)\n",
        "    out = fully_conn(out, num_outputs = 16)\n",
        "    #    Set this to the number of classes\n",
        "    # Function Definition from Above:\n",
        "    out = output(out, num_outputs = 10)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "##############################\n",
        "## Build the Neural Network ##\n",
        "##############################\n",
        "\n",
        "# Remove previous weights, bias, inputs, etc..\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Inputs\n",
        "x = neural_net_image_input((32, 32, 3))\n",
        "y = neural_net_label_input(10)\n",
        "keep_prob = neural_net_keep_prob_input()\n",
        "\n",
        "# Model\n",
        "logits = conv_net(x, keep_prob)\n",
        "\n",
        "# Name logits Tensor, so that is can be loaded from disk after training\n",
        "logits = tf.identity(logits, name='logits')\n",
        "\n",
        "# Loss and Optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-14-2f9c98f02500>:19: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-14-2f9c98f02500>:49: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r8OgwqSKffeg"
      },
      "source": [
        "## Train the Neural Network\n",
        "### Single Optimization\n",
        "Implement the function `train_neural_network` to do a single optimization.  The optimization uses `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
        "* `x` for image input\n",
        "* `y` for labels\n",
        "* `keep_prob` for keep probability for dropout\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WQf2Slxgffek",
        "colab": {}
      },
      "source": [
        "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
        "    \"\"\"\n",
        "    Optimize the session on a batch of images and labels\n",
        "    : session: Current TensorFlow session\n",
        "    : optimizer: TensorFlow optimizer function\n",
        "    : keep_probability: keep probability\n",
        "    : feature_batch: Batch of Numpy image data\n",
        "    : label_batch: Batch of Numpy label data\n",
        "    \"\"\"\n",
        "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch,keep_prob:keep_probability})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4GjcHeJffev"
      },
      "source": [
        "### Show Stats\n",
        "Implement the function `print_stats` to print loss and validation accuracy.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fm3vpebsfffN",
        "colab": {}
      },
      "source": [
        "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
        "    \"\"\"\n",
        "    Print information about loss and validation accuracy\n",
        "    : session: Current TensorFlow session\n",
        "    : feature_batch: Batch of Numpy image data\n",
        "    : label_batch: Batch of Numpy label data\n",
        "    : cost: TensorFlow cost function\n",
        "    : accuracy: TensorFlow accuracy function\n",
        "    \"\"\"\n",
        "    valid_acc = sess.run(accuracy, feed_dict={\n",
        "                x: valid_features,\n",
        "                y: valid_labels,\n",
        "                keep_prob: 1.})\n",
        "    train_acc = sess.run(accuracy, feed_dict={\n",
        "                x: feature_batch,\n",
        "                y: label_batch,\n",
        "                keep_prob: 1.})\n",
        "    train_cost = sess.run(cost, feed_dict={\n",
        "                x: feature_batch,\n",
        "                y: label_batch,\n",
        "                keep_prob: 1.})\n",
        "    print('Cost {::>8.5f}, Accuracy on Training {:.4f} -'\n",
        "                  'Validation Accuracy: {:.4f}'.format(\n",
        "                train_cost,\n",
        "                train_acc,\n",
        "                valid_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0IAZd6gvfffX"
      },
      "source": [
        "### Hyperparameters\n",
        "The following parameters are tuned:\n",
        "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
        "* Set `batch_size` \n",
        "* Set `keep_probability` to the probability of keeping a node using dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KT9OrDjufffp",
        "colab": {}
      },
      "source": [
        "epochs = 50\n",
        "batch_size = 256\n",
        "keep_probability = 0.6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "535N9K3lfff5"
      },
      "source": [
        "### Train on a Single CIFAR-10 Batch\n",
        "Instead of training the neural network on all the CIFAR-10 batches of data, a single batch has been used. This is for saving time while iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, the model can be run on all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBHJG9twfff9",
        "outputId": "ffb54b8b-aedb-4f34-bab3-2a200c7c190e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "print('Checking the Training on a Single Batch...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        batch_i = 1\n",
        "        for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the Training on a Single Batch...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Cost :2.27425, Accuracy on Training 0.1250 -Validation Accuracy: 0.1190\n",
            "Epoch  2, CIFAR-10 Batch 1:  Cost :2.25925, Accuracy on Training 0.2000 -Validation Accuracy: 0.1812\n",
            "Epoch  3, CIFAR-10 Batch 1:  Cost :2.22405, Accuracy on Training 0.1750 -Validation Accuracy: 0.2162\n",
            "Epoch  4, CIFAR-10 Batch 1:  Cost :2.19201, Accuracy on Training 0.2250 -Validation Accuracy: 0.2124\n",
            "Epoch  5, CIFAR-10 Batch 1:  Cost :2.15159, Accuracy on Training 0.2250 -Validation Accuracy: 0.2200\n",
            "Epoch  6, CIFAR-10 Batch 1:  Cost :2.11572, Accuracy on Training 0.1500 -Validation Accuracy: 0.2478\n",
            "Epoch  7, CIFAR-10 Batch 1:  Cost :2.06256, Accuracy on Training 0.3250 -Validation Accuracy: 0.2668\n",
            "Epoch  8, CIFAR-10 Batch 1:  Cost :2.00049, Accuracy on Training 0.2500 -Validation Accuracy: 0.2796\n",
            "Epoch  9, CIFAR-10 Batch 1:  Cost :1.94495, Accuracy on Training 0.3500 -Validation Accuracy: 0.3084\n",
            "Epoch 10, CIFAR-10 Batch 1:  Cost :1.87209, Accuracy on Training 0.4000 -Validation Accuracy: 0.3280\n",
            "Epoch 11, CIFAR-10 Batch 1:  Cost :1.80072, Accuracy on Training 0.3750 -Validation Accuracy: 0.3370\n",
            "Epoch 12, CIFAR-10 Batch 1:  Cost :1.78120, Accuracy on Training 0.4000 -Validation Accuracy: 0.3382\n",
            "Epoch 13, CIFAR-10 Batch 1:  Cost :1.76177, Accuracy on Training 0.4750 -Validation Accuracy: 0.3498\n",
            "Epoch 14, CIFAR-10 Batch 1:  Cost :1.70186, Accuracy on Training 0.4750 -Validation Accuracy: 0.3698\n",
            "Epoch 15, CIFAR-10 Batch 1:  Cost :1.65459, Accuracy on Training 0.5000 -Validation Accuracy: 0.3778\n",
            "Epoch 16, CIFAR-10 Batch 1:  Cost :1.60585, Accuracy on Training 0.4750 -Validation Accuracy: 0.3916\n",
            "Epoch 17, CIFAR-10 Batch 1:  Cost :1.51905, Accuracy on Training 0.5500 -Validation Accuracy: 0.4086\n",
            "Epoch 18, CIFAR-10 Batch 1:  Cost :1.46947, Accuracy on Training 0.5500 -Validation Accuracy: 0.4144\n",
            "Epoch 19, CIFAR-10 Batch 1:  Cost :1.43568, Accuracy on Training 0.6000 -Validation Accuracy: 0.4126\n",
            "Epoch 20, CIFAR-10 Batch 1:  Cost :1.35279, Accuracy on Training 0.5000 -Validation Accuracy: 0.4374\n",
            "Epoch 21, CIFAR-10 Batch 1:  Cost :1.34535, Accuracy on Training 0.6000 -Validation Accuracy: 0.4260\n",
            "Epoch 22, CIFAR-10 Batch 1:  Cost :1.28656, Accuracy on Training 0.6500 -Validation Accuracy: 0.4302\n",
            "Epoch 23, CIFAR-10 Batch 1:  Cost :1.27816, Accuracy on Training 0.6750 -Validation Accuracy: 0.4422\n",
            "Epoch 24, CIFAR-10 Batch 1:  Cost :1.21395, Accuracy on Training 0.6750 -Validation Accuracy: 0.4478\n",
            "Epoch 25, CIFAR-10 Batch 1:  Cost :1.18691, Accuracy on Training 0.6500 -Validation Accuracy: 0.4256\n",
            "Epoch 26, CIFAR-10 Batch 1:  Cost :1.23596, Accuracy on Training 0.6250 -Validation Accuracy: 0.4178\n",
            "Epoch 27, CIFAR-10 Batch 1:  Cost :1.10928, Accuracy on Training 0.6250 -Validation Accuracy: 0.4666\n",
            "Epoch 28, CIFAR-10 Batch 1:  Cost :1.08624, Accuracy on Training 0.6250 -Validation Accuracy: 0.4704\n",
            "Epoch 29, CIFAR-10 Batch 1:  Cost :1.01812, Accuracy on Training 0.6250 -Validation Accuracy: 0.4706\n",
            "Epoch 30, CIFAR-10 Batch 1:  Cost :0.97352, Accuracy on Training 0.6750 -Validation Accuracy: 0.4848\n",
            "Epoch 31, CIFAR-10 Batch 1:  Cost :0.95017, Accuracy on Training 0.6750 -Validation Accuracy: 0.4672\n",
            "Epoch 32, CIFAR-10 Batch 1:  Cost :0.92322, Accuracy on Training 0.7250 -Validation Accuracy: 0.4744\n",
            "Epoch 33, CIFAR-10 Batch 1:  Cost :0.94385, Accuracy on Training 0.7250 -Validation Accuracy: 0.4780\n",
            "Epoch 34, CIFAR-10 Batch 1:  Cost :0.88975, Accuracy on Training 0.6750 -Validation Accuracy: 0.4926\n",
            "Epoch 35, CIFAR-10 Batch 1:  Cost :0.87120, Accuracy on Training 0.6750 -Validation Accuracy: 0.4860\n",
            "Epoch 36, CIFAR-10 Batch 1:  Cost :0.84944, Accuracy on Training 0.6250 -Validation Accuracy: 0.4924\n",
            "Epoch 37, CIFAR-10 Batch 1:  Cost :0.80519, Accuracy on Training 0.7250 -Validation Accuracy: 0.4846\n",
            "Epoch 38, CIFAR-10 Batch 1:  Cost :0.81739, Accuracy on Training 0.6750 -Validation Accuracy: 0.5036\n",
            "Epoch 39, CIFAR-10 Batch 1:  Cost :0.80635, Accuracy on Training 0.7000 -Validation Accuracy: 0.4896\n",
            "Epoch 40, CIFAR-10 Batch 1:  Cost :0.77116, Accuracy on Training 0.7500 -Validation Accuracy: 0.4962\n",
            "Epoch 41, CIFAR-10 Batch 1:  Cost :0.74204, Accuracy on Training 0.7250 -Validation Accuracy: 0.5096\n",
            "Epoch 42, CIFAR-10 Batch 1:  Cost :0.71435, Accuracy on Training 0.8000 -Validation Accuracy: 0.5002\n",
            "Epoch 43, CIFAR-10 Batch 1:  Cost :0.69858, Accuracy on Training 0.7750 -Validation Accuracy: 0.5050\n",
            "Epoch 44, CIFAR-10 Batch 1:  Cost :0.71671, Accuracy on Training 0.8000 -Validation Accuracy: 0.5088\n",
            "Epoch 45, CIFAR-10 Batch 1:  Cost :0.69706, Accuracy on Training 0.7750 -Validation Accuracy: 0.5106\n",
            "Epoch 46, CIFAR-10 Batch 1:  Cost :0.70810, Accuracy on Training 0.7500 -Validation Accuracy: 0.5100\n",
            "Epoch 47, CIFAR-10 Batch 1:  Cost :0.66659, Accuracy on Training 0.7750 -Validation Accuracy: 0.5066\n",
            "Epoch 48, CIFAR-10 Batch 1:  Cost :0.67779, Accuracy on Training 0.7500 -Validation Accuracy: 0.5046\n",
            "Epoch 49, CIFAR-10 Batch 1:  Cost :0.62819, Accuracy on Training 0.7750 -Validation Accuracy: 0.5140\n",
            "Epoch 50, CIFAR-10 Batch 1:  Cost :0.64839, Accuracy on Training 0.8000 -Validation Accuracy: 0.5264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM28G5ZrffgS"
      },
      "source": [
        "### Fully Train the Model\n",
        "As the accuracy is good now with a single CIFAR-10 batch, let's try it with all five batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qNwYN5ZZffgU",
        "outputId": "53ff11ce-f07e-42c4-d9d9-e499cdbc03f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4368
        }
      },
      "source": [
        "save_model_path = './image_classification'\n",
        "\n",
        "print('Training...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        # Loop over all batches\n",
        "        n_batches = 5\n",
        "        for batch_i in range(1, n_batches + 1):\n",
        "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "            \n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    save_path = saver.save(sess, save_model_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Cost :2.29577, Accuracy on Training 0.1750 -Validation Accuracy: 0.1292\n",
            "Epoch  1, CIFAR-10 Batch 2:  Cost :2.28283, Accuracy on Training 0.1500 -Validation Accuracy: 0.1688\n",
            "Epoch  1, CIFAR-10 Batch 3:  Cost :2.21792, Accuracy on Training 0.3000 -Validation Accuracy: 0.1782\n",
            "Epoch  1, CIFAR-10 Batch 4:  Cost :2.19802, Accuracy on Training 0.1500 -Validation Accuracy: 0.1846\n",
            "Epoch  1, CIFAR-10 Batch 5:  Cost :2.16286, Accuracy on Training 0.2250 -Validation Accuracy: 0.1932\n",
            "Epoch  2, CIFAR-10 Batch 1:  Cost :2.21760, Accuracy on Training 0.1250 -Validation Accuracy: 0.2046\n",
            "Epoch  2, CIFAR-10 Batch 2:  Cost :2.12158, Accuracy on Training 0.1500 -Validation Accuracy: 0.2132\n",
            "Epoch  2, CIFAR-10 Batch 3:  Cost :1.94688, Accuracy on Training 0.4000 -Validation Accuracy: 0.2498\n",
            "Epoch  2, CIFAR-10 Batch 4:  Cost :1.98614, Accuracy on Training 0.2750 -Validation Accuracy: 0.2658\n",
            "Epoch  2, CIFAR-10 Batch 5:  Cost :1.97360, Accuracy on Training 0.2750 -Validation Accuracy: 0.2818\n",
            "Epoch  3, CIFAR-10 Batch 1:  Cost :2.13639, Accuracy on Training 0.3000 -Validation Accuracy: 0.2650\n",
            "Epoch  3, CIFAR-10 Batch 2:  Cost :1.99791, Accuracy on Training 0.3000 -Validation Accuracy: 0.3142\n",
            "Epoch  3, CIFAR-10 Batch 3:  Cost :1.64565, Accuracy on Training 0.4250 -Validation Accuracy: 0.3018\n",
            "Epoch  3, CIFAR-10 Batch 4:  Cost :1.79406, Accuracy on Training 0.2250 -Validation Accuracy: 0.3324\n",
            "Epoch  3, CIFAR-10 Batch 5:  Cost :1.75649, Accuracy on Training 0.3500 -Validation Accuracy: 0.3288\n",
            "Epoch  4, CIFAR-10 Batch 1:  Cost :1.94003, Accuracy on Training 0.2750 -Validation Accuracy: 0.3322\n",
            "Epoch  4, CIFAR-10 Batch 2:  Cost :1.88599, Accuracy on Training 0.3500 -Validation Accuracy: 0.3342\n",
            "Epoch  4, CIFAR-10 Batch 3:  Cost :1.50288, Accuracy on Training 0.3500 -Validation Accuracy: 0.3274\n",
            "Epoch  4, CIFAR-10 Batch 4:  Cost :1.71204, Accuracy on Training 0.3250 -Validation Accuracy: 0.3666\n",
            "Epoch  4, CIFAR-10 Batch 5:  Cost :1.71841, Accuracy on Training 0.4000 -Validation Accuracy: 0.3436\n",
            "Epoch  5, CIFAR-10 Batch 1:  Cost :1.79783, Accuracy on Training 0.3250 -Validation Accuracy: 0.3592\n",
            "Epoch  5, CIFAR-10 Batch 2:  Cost :1.73889, Accuracy on Training 0.5250 -Validation Accuracy: 0.3562\n",
            "Epoch  5, CIFAR-10 Batch 3:  Cost :1.39510, Accuracy on Training 0.4750 -Validation Accuracy: 0.3634\n",
            "Epoch  5, CIFAR-10 Batch 4:  Cost :1.64232, Accuracy on Training 0.3500 -Validation Accuracy: 0.3734\n",
            "Epoch  5, CIFAR-10 Batch 5:  Cost :1.65496, Accuracy on Training 0.3750 -Validation Accuracy: 0.3414\n",
            "Epoch  6, CIFAR-10 Batch 1:  Cost :1.74819, Accuracy on Training 0.3750 -Validation Accuracy: 0.3828\n",
            "Epoch  6, CIFAR-10 Batch 2:  Cost :1.69747, Accuracy on Training 0.5000 -Validation Accuracy: 0.3964\n",
            "Epoch  6, CIFAR-10 Batch 3:  Cost :1.37404, Accuracy on Training 0.5000 -Validation Accuracy: 0.3594\n",
            "Epoch  6, CIFAR-10 Batch 4:  Cost :1.59520, Accuracy on Training 0.3500 -Validation Accuracy: 0.3972\n",
            "Epoch  6, CIFAR-10 Batch 5:  Cost :1.60005, Accuracy on Training 0.4250 -Validation Accuracy: 0.4038\n",
            "Epoch  7, CIFAR-10 Batch 1:  Cost :1.58604, Accuracy on Training 0.4000 -Validation Accuracy: 0.4076\n",
            "Epoch  7, CIFAR-10 Batch 2:  Cost :1.67142, Accuracy on Training 0.4250 -Validation Accuracy: 0.4128\n",
            "Epoch  7, CIFAR-10 Batch 3:  Cost :1.27305, Accuracy on Training 0.5250 -Validation Accuracy: 0.4042\n",
            "Epoch  7, CIFAR-10 Batch 4:  Cost :1.50770, Accuracy on Training 0.3500 -Validation Accuracy: 0.4184\n",
            "Epoch  7, CIFAR-10 Batch 5:  Cost :1.56850, Accuracy on Training 0.4000 -Validation Accuracy: 0.4038\n",
            "Epoch  8, CIFAR-10 Batch 1:  Cost :1.49385, Accuracy on Training 0.4000 -Validation Accuracy: 0.4246\n",
            "Epoch  8, CIFAR-10 Batch 2:  Cost :1.58910, Accuracy on Training 0.4500 -Validation Accuracy: 0.4116\n",
            "Epoch  8, CIFAR-10 Batch 3:  Cost :1.33673, Accuracy on Training 0.4250 -Validation Accuracy: 0.4044\n",
            "Epoch  8, CIFAR-10 Batch 4:  Cost :1.44841, Accuracy on Training 0.3750 -Validation Accuracy: 0.4298\n",
            "Epoch  8, CIFAR-10 Batch 5:  Cost :1.42949, Accuracy on Training 0.3750 -Validation Accuracy: 0.4180\n",
            "Epoch  9, CIFAR-10 Batch 1:  Cost :1.41218, Accuracy on Training 0.3750 -Validation Accuracy: 0.4378\n",
            "Epoch  9, CIFAR-10 Batch 2:  Cost :1.51074, Accuracy on Training 0.4750 -Validation Accuracy: 0.4368\n",
            "Epoch  9, CIFAR-10 Batch 3:  Cost :1.20592, Accuracy on Training 0.5250 -Validation Accuracy: 0.4506\n",
            "Epoch  9, CIFAR-10 Batch 4:  Cost :1.35920, Accuracy on Training 0.4750 -Validation Accuracy: 0.4546\n",
            "Epoch  9, CIFAR-10 Batch 5:  Cost :1.36586, Accuracy on Training 0.5250 -Validation Accuracy: 0.4148\n",
            "Epoch 10, CIFAR-10 Batch 1:  Cost :1.33726, Accuracy on Training 0.4750 -Validation Accuracy: 0.4370\n",
            "Epoch 10, CIFAR-10 Batch 2:  Cost :1.36646, Accuracy on Training 0.5000 -Validation Accuracy: 0.4582\n",
            "Epoch 10, CIFAR-10 Batch 3:  Cost :1.16658, Accuracy on Training 0.5000 -Validation Accuracy: 0.4558\n",
            "Epoch 10, CIFAR-10 Batch 4:  Cost :1.30766, Accuracy on Training 0.5000 -Validation Accuracy: 0.4680\n",
            "Epoch 10, CIFAR-10 Batch 5:  Cost :1.31333, Accuracy on Training 0.5750 -Validation Accuracy: 0.4684\n",
            "Epoch 11, CIFAR-10 Batch 1:  Cost :1.40513, Accuracy on Training 0.4000 -Validation Accuracy: 0.4334\n",
            "Epoch 11, CIFAR-10 Batch 2:  Cost :1.34559, Accuracy on Training 0.5000 -Validation Accuracy: 0.4742\n",
            "Epoch 11, CIFAR-10 Batch 3:  Cost :1.15471, Accuracy on Training 0.4500 -Validation Accuracy: 0.4736\n",
            "Epoch 11, CIFAR-10 Batch 4:  Cost :1.25591, Accuracy on Training 0.5500 -Validation Accuracy: 0.4588\n",
            "Epoch 11, CIFAR-10 Batch 5:  Cost :1.27785, Accuracy on Training 0.6000 -Validation Accuracy: 0.4786\n",
            "Epoch 12, CIFAR-10 Batch 1:  Cost :1.39873, Accuracy on Training 0.4500 -Validation Accuracy: 0.4558\n",
            "Epoch 12, CIFAR-10 Batch 2:  Cost :1.27610, Accuracy on Training 0.5000 -Validation Accuracy: 0.4852\n",
            "Epoch 12, CIFAR-10 Batch 3:  Cost :1.14053, Accuracy on Training 0.5500 -Validation Accuracy: 0.4544\n",
            "Epoch 12, CIFAR-10 Batch 4:  Cost :1.16221, Accuracy on Training 0.5500 -Validation Accuracy: 0.4798\n",
            "Epoch 12, CIFAR-10 Batch 5:  Cost :1.24456, Accuracy on Training 0.5500 -Validation Accuracy: 0.4732\n",
            "Epoch 13, CIFAR-10 Batch 1:  Cost :1.24923, Accuracy on Training 0.5250 -Validation Accuracy: 0.4928\n",
            "Epoch 13, CIFAR-10 Batch 2:  Cost :1.25260, Accuracy on Training 0.4750 -Validation Accuracy: 0.4812\n",
            "Epoch 13, CIFAR-10 Batch 3:  Cost :1.11721, Accuracy on Training 0.5750 -Validation Accuracy: 0.4784\n",
            "Epoch 13, CIFAR-10 Batch 4:  Cost :1.15624, Accuracy on Training 0.6250 -Validation Accuracy: 0.4930\n",
            "Epoch 13, CIFAR-10 Batch 5:  Cost :1.17572, Accuracy on Training 0.5500 -Validation Accuracy: 0.4648\n",
            "Epoch 14, CIFAR-10 Batch 1:  Cost :1.22493, Accuracy on Training 0.5000 -Validation Accuracy: 0.4904\n",
            "Epoch 14, CIFAR-10 Batch 2:  Cost :1.26307, Accuracy on Training 0.5500 -Validation Accuracy: 0.4916\n",
            "Epoch 14, CIFAR-10 Batch 3:  Cost :1.07377, Accuracy on Training 0.6000 -Validation Accuracy: 0.4864\n",
            "Epoch 14, CIFAR-10 Batch 4:  Cost :1.13009, Accuracy on Training 0.5750 -Validation Accuracy: 0.5002\n",
            "Epoch 14, CIFAR-10 Batch 5:  Cost :1.16983, Accuracy on Training 0.6250 -Validation Accuracy: 0.4910\n",
            "Epoch 15, CIFAR-10 Batch 1:  Cost :1.14828, Accuracy on Training 0.4750 -Validation Accuracy: 0.5150\n",
            "Epoch 15, CIFAR-10 Batch 2:  Cost :1.19803, Accuracy on Training 0.5000 -Validation Accuracy: 0.4952\n",
            "Epoch 15, CIFAR-10 Batch 3:  Cost :1.04442, Accuracy on Training 0.6250 -Validation Accuracy: 0.4906\n",
            "Epoch 15, CIFAR-10 Batch 4:  Cost :1.11299, Accuracy on Training 0.6500 -Validation Accuracy: 0.5186\n",
            "Epoch 15, CIFAR-10 Batch 5:  Cost :1.14505, Accuracy on Training 0.5750 -Validation Accuracy: 0.4826\n",
            "Epoch 16, CIFAR-10 Batch 1:  Cost :1.23834, Accuracy on Training 0.5000 -Validation Accuracy: 0.5022\n",
            "Epoch 16, CIFAR-10 Batch 2:  Cost :1.13052, Accuracy on Training 0.5750 -Validation Accuracy: 0.5164\n",
            "Epoch 16, CIFAR-10 Batch 3:  Cost :1.00708, Accuracy on Training 0.6250 -Validation Accuracy: 0.5124\n",
            "Epoch 16, CIFAR-10 Batch 4:  Cost :1.07397, Accuracy on Training 0.5750 -Validation Accuracy: 0.5160\n",
            "Epoch 16, CIFAR-10 Batch 5:  Cost :1.11842, Accuracy on Training 0.6000 -Validation Accuracy: 0.5068\n",
            "Epoch 17, CIFAR-10 Batch 1:  Cost :1.13774, Accuracy on Training 0.5500 -Validation Accuracy: 0.5204\n",
            "Epoch 17, CIFAR-10 Batch 2:  Cost :1.08098, Accuracy on Training 0.6000 -Validation Accuracy: 0.5152\n",
            "Epoch 17, CIFAR-10 Batch 3:  Cost :0.99378, Accuracy on Training 0.6000 -Validation Accuracy: 0.5052\n",
            "Epoch 17, CIFAR-10 Batch 4:  Cost :1.07404, Accuracy on Training 0.6000 -Validation Accuracy: 0.5162\n",
            "Epoch 17, CIFAR-10 Batch 5:  Cost :1.09859, Accuracy on Training 0.6250 -Validation Accuracy: 0.5116\n",
            "Epoch 18, CIFAR-10 Batch 1:  Cost :1.07398, Accuracy on Training 0.6000 -Validation Accuracy: 0.5284\n",
            "Epoch 18, CIFAR-10 Batch 2:  Cost :1.06995, Accuracy on Training 0.5500 -Validation Accuracy: 0.5238\n",
            "Epoch 18, CIFAR-10 Batch 3:  Cost :0.97084, Accuracy on Training 0.6000 -Validation Accuracy: 0.5126\n",
            "Epoch 18, CIFAR-10 Batch 4:  Cost :0.98440, Accuracy on Training 0.6500 -Validation Accuracy: 0.5276\n",
            "Epoch 18, CIFAR-10 Batch 5:  Cost :1.05625, Accuracy on Training 0.6500 -Validation Accuracy: 0.5242\n",
            "Epoch 19, CIFAR-10 Batch 1:  Cost :1.01498, Accuracy on Training 0.6000 -Validation Accuracy: 0.5354\n",
            "Epoch 19, CIFAR-10 Batch 2:  Cost :1.08214, Accuracy on Training 0.6000 -Validation Accuracy: 0.5156\n",
            "Epoch 19, CIFAR-10 Batch 3:  Cost :0.88645, Accuracy on Training 0.6750 -Validation Accuracy: 0.5136\n",
            "Epoch 19, CIFAR-10 Batch 4:  Cost :0.97506, Accuracy on Training 0.6250 -Validation Accuracy: 0.5286\n",
            "Epoch 19, CIFAR-10 Batch 5:  Cost :1.01922, Accuracy on Training 0.6500 -Validation Accuracy: 0.5096\n",
            "Epoch 20, CIFAR-10 Batch 1:  Cost :1.01036, Accuracy on Training 0.5750 -Validation Accuracy: 0.5370\n",
            "Epoch 20, CIFAR-10 Batch 2:  Cost :1.01735, Accuracy on Training 0.6500 -Validation Accuracy: 0.5346\n",
            "Epoch 20, CIFAR-10 Batch 3:  Cost :0.89678, Accuracy on Training 0.7000 -Validation Accuracy: 0.5172\n",
            "Epoch 20, CIFAR-10 Batch 4:  Cost :0.96431, Accuracy on Training 0.6750 -Validation Accuracy: 0.5392\n",
            "Epoch 20, CIFAR-10 Batch 5:  Cost :1.05882, Accuracy on Training 0.6500 -Validation Accuracy: 0.5176\n",
            "Epoch 21, CIFAR-10 Batch 1:  Cost :0.98809, Accuracy on Training 0.6000 -Validation Accuracy: 0.5386\n",
            "Epoch 21, CIFAR-10 Batch 2:  Cost :0.99392, Accuracy on Training 0.6750 -Validation Accuracy: 0.5264\n",
            "Epoch 21, CIFAR-10 Batch 3:  Cost :0.82614, Accuracy on Training 0.7750 -Validation Accuracy: 0.5196\n",
            "Epoch 21, CIFAR-10 Batch 4:  Cost :0.94374, Accuracy on Training 0.6750 -Validation Accuracy: 0.5354\n",
            "Epoch 21, CIFAR-10 Batch 5:  Cost :0.95746, Accuracy on Training 0.7000 -Validation Accuracy: 0.5252\n",
            "Epoch 22, CIFAR-10 Batch 1:  Cost :0.94931, Accuracy on Training 0.5750 -Validation Accuracy: 0.5408\n",
            "Epoch 22, CIFAR-10 Batch 2:  Cost :0.94811, Accuracy on Training 0.7250 -Validation Accuracy: 0.5376\n",
            "Epoch 22, CIFAR-10 Batch 3:  Cost :0.82265, Accuracy on Training 0.7000 -Validation Accuracy: 0.5096\n",
            "Epoch 22, CIFAR-10 Batch 4:  Cost :0.91279, Accuracy on Training 0.7250 -Validation Accuracy: 0.5308\n",
            "Epoch 22, CIFAR-10 Batch 5:  Cost :0.97536, Accuracy on Training 0.7250 -Validation Accuracy: 0.5178\n",
            "Epoch 23, CIFAR-10 Batch 1:  Cost :0.94487, Accuracy on Training 0.6500 -Validation Accuracy: 0.5380\n",
            "Epoch 23, CIFAR-10 Batch 2:  Cost :0.93780, Accuracy on Training 0.7000 -Validation Accuracy: 0.5390\n",
            "Epoch 23, CIFAR-10 Batch 3:  Cost :0.78959, Accuracy on Training 0.7500 -Validation Accuracy: 0.5424\n",
            "Epoch 23, CIFAR-10 Batch 4:  Cost :0.92460, Accuracy on Training 0.6250 -Validation Accuracy: 0.5416\n",
            "Epoch 23, CIFAR-10 Batch 5:  Cost :0.95522, Accuracy on Training 0.6750 -Validation Accuracy: 0.5344\n",
            "Epoch 24, CIFAR-10 Batch 1:  Cost :0.92059, Accuracy on Training 0.6500 -Validation Accuracy: 0.5460\n",
            "Epoch 24, CIFAR-10 Batch 2:  Cost :0.93693, Accuracy on Training 0.7750 -Validation Accuracy: 0.5436\n",
            "Epoch 24, CIFAR-10 Batch 3:  Cost :0.80133, Accuracy on Training 0.7500 -Validation Accuracy: 0.5352\n",
            "Epoch 24, CIFAR-10 Batch 4:  Cost :0.85157, Accuracy on Training 0.7500 -Validation Accuracy: 0.5604\n",
            "Epoch 24, CIFAR-10 Batch 5:  Cost :0.92461, Accuracy on Training 0.6750 -Validation Accuracy: 0.5470\n",
            "Epoch 25, CIFAR-10 Batch 1:  Cost :0.91794, Accuracy on Training 0.7000 -Validation Accuracy: 0.5342\n",
            "Epoch 25, CIFAR-10 Batch 2:  Cost :0.92905, Accuracy on Training 0.7500 -Validation Accuracy: 0.5506\n",
            "Epoch 25, CIFAR-10 Batch 3:  Cost :0.73334, Accuracy on Training 0.7500 -Validation Accuracy: 0.5458\n",
            "Epoch 25, CIFAR-10 Batch 4:  Cost :0.83449, Accuracy on Training 0.7250 -Validation Accuracy: 0.5488\n",
            "Epoch 25, CIFAR-10 Batch 5:  Cost :0.87737, Accuracy on Training 0.7000 -Validation Accuracy: 0.5544\n",
            "Epoch 26, CIFAR-10 Batch 1:  Cost :0.88109, Accuracy on Training 0.6500 -Validation Accuracy: 0.5674\n",
            "Epoch 26, CIFAR-10 Batch 2:  Cost :0.80430, Accuracy on Training 0.8250 -Validation Accuracy: 0.5602\n",
            "Epoch 26, CIFAR-10 Batch 3:  Cost :0.76746, Accuracy on Training 0.7250 -Validation Accuracy: 0.5580\n",
            "Epoch 26, CIFAR-10 Batch 4:  Cost :0.82030, Accuracy on Training 0.7500 -Validation Accuracy: 0.5644\n",
            "Epoch 26, CIFAR-10 Batch 5:  Cost :0.87408, Accuracy on Training 0.7500 -Validation Accuracy: 0.5554\n",
            "Epoch 27, CIFAR-10 Batch 1:  Cost :0.85751, Accuracy on Training 0.5750 -Validation Accuracy: 0.5718\n",
            "Epoch 27, CIFAR-10 Batch 2:  Cost :0.84440, Accuracy on Training 0.7750 -Validation Accuracy: 0.5656\n",
            "Epoch 27, CIFAR-10 Batch 3:  Cost :0.73290, Accuracy on Training 0.6750 -Validation Accuracy: 0.5570\n",
            "Epoch 27, CIFAR-10 Batch 4:  Cost :0.77290, Accuracy on Training 0.7250 -Validation Accuracy: 0.5712\n",
            "Epoch 27, CIFAR-10 Batch 5:  Cost :0.82426, Accuracy on Training 0.7500 -Validation Accuracy: 0.5596\n",
            "Epoch 28, CIFAR-10 Batch 1:  Cost :0.81945, Accuracy on Training 0.6500 -Validation Accuracy: 0.5772\n",
            "Epoch 28, CIFAR-10 Batch 2:  Cost :0.76778, Accuracy on Training 0.7750 -Validation Accuracy: 0.5634\n",
            "Epoch 28, CIFAR-10 Batch 3:  Cost :0.70162, Accuracy on Training 0.7500 -Validation Accuracy: 0.5634\n",
            "Epoch 28, CIFAR-10 Batch 4:  Cost :0.73891, Accuracy on Training 0.7250 -Validation Accuracy: 0.5722\n",
            "Epoch 28, CIFAR-10 Batch 5:  Cost :0.82947, Accuracy on Training 0.7250 -Validation Accuracy: 0.5544\n",
            "Epoch 29, CIFAR-10 Batch 1:  Cost :0.80816, Accuracy on Training 0.7000 -Validation Accuracy: 0.5770\n",
            "Epoch 29, CIFAR-10 Batch 2:  Cost :0.73057, Accuracy on Training 0.8000 -Validation Accuracy: 0.5692\n",
            "Epoch 29, CIFAR-10 Batch 3:  Cost :0.70762, Accuracy on Training 0.7750 -Validation Accuracy: 0.5550\n",
            "Epoch 29, CIFAR-10 Batch 4:  Cost :0.71302, Accuracy on Training 0.7250 -Validation Accuracy: 0.5764\n",
            "Epoch 29, CIFAR-10 Batch 5:  Cost :0.80253, Accuracy on Training 0.7500 -Validation Accuracy: 0.5698\n",
            "Epoch 30, CIFAR-10 Batch 1:  Cost :0.78540, Accuracy on Training 0.7250 -Validation Accuracy: 0.5758\n",
            "Epoch 30, CIFAR-10 Batch 2:  Cost :0.75005, Accuracy on Training 0.8000 -Validation Accuracy: 0.5770\n",
            "Epoch 30, CIFAR-10 Batch 3:  Cost :0.69054, Accuracy on Training 0.7750 -Validation Accuracy: 0.5592\n",
            "Epoch 30, CIFAR-10 Batch 4:  Cost :0.74882, Accuracy on Training 0.7750 -Validation Accuracy: 0.5916\n",
            "Epoch 30, CIFAR-10 Batch 5:  Cost :0.79251, Accuracy on Training 0.7500 -Validation Accuracy: 0.5842\n",
            "Epoch 31, CIFAR-10 Batch 1:  Cost :0.79909, Accuracy on Training 0.6750 -Validation Accuracy: 0.5866\n",
            "Epoch 31, CIFAR-10 Batch 2:  Cost :0.74635, Accuracy on Training 0.8250 -Validation Accuracy: 0.5776\n",
            "Epoch 31, CIFAR-10 Batch 3:  Cost :0.63262, Accuracy on Training 0.7750 -Validation Accuracy: 0.5636\n",
            "Epoch 31, CIFAR-10 Batch 4:  Cost :0.67530, Accuracy on Training 0.8250 -Validation Accuracy: 0.5862\n",
            "Epoch 31, CIFAR-10 Batch 5:  Cost :0.73926, Accuracy on Training 0.8250 -Validation Accuracy: 0.5666\n",
            "Epoch 32, CIFAR-10 Batch 1:  Cost :0.73812, Accuracy on Training 0.7250 -Validation Accuracy: 0.5904\n",
            "Epoch 32, CIFAR-10 Batch 2:  Cost :0.72644, Accuracy on Training 0.8000 -Validation Accuracy: 0.5690\n",
            "Epoch 32, CIFAR-10 Batch 3:  Cost :0.65418, Accuracy on Training 0.8250 -Validation Accuracy: 0.5434\n",
            "Epoch 32, CIFAR-10 Batch 4:  Cost :0.68496, Accuracy on Training 0.8000 -Validation Accuracy: 0.5884\n",
            "Epoch 32, CIFAR-10 Batch 5:  Cost :0.74460, Accuracy on Training 0.7500 -Validation Accuracy: 0.5840\n",
            "Epoch 33, CIFAR-10 Batch 1:  Cost :0.74724, Accuracy on Training 0.6750 -Validation Accuracy: 0.5858\n",
            "Epoch 33, CIFAR-10 Batch 2:  Cost :0.67698, Accuracy on Training 0.8250 -Validation Accuracy: 0.5690\n",
            "Epoch 33, CIFAR-10 Batch 3:  Cost :0.62762, Accuracy on Training 0.8000 -Validation Accuracy: 0.5618\n",
            "Epoch 33, CIFAR-10 Batch 4:  Cost :0.68847, Accuracy on Training 0.8000 -Validation Accuracy: 0.5856\n",
            "Epoch 33, CIFAR-10 Batch 5:  Cost :0.73680, Accuracy on Training 0.7500 -Validation Accuracy: 0.5846\n",
            "Epoch 34, CIFAR-10 Batch 1:  Cost :0.72665, Accuracy on Training 0.7250 -Validation Accuracy: 0.5922\n",
            "Epoch 34, CIFAR-10 Batch 2:  Cost :0.65565, Accuracy on Training 0.8750 -Validation Accuracy: 0.5734\n",
            "Epoch 34, CIFAR-10 Batch 3:  Cost :0.62258, Accuracy on Training 0.8000 -Validation Accuracy: 0.5458\n",
            "Epoch 34, CIFAR-10 Batch 4:  Cost :0.68648, Accuracy on Training 0.7500 -Validation Accuracy: 0.5982\n",
            "Epoch 34, CIFAR-10 Batch 5:  Cost :0.73941, Accuracy on Training 0.7250 -Validation Accuracy: 0.5784\n",
            "Epoch 35, CIFAR-10 Batch 1:  Cost :0.69708, Accuracy on Training 0.7750 -Validation Accuracy: 0.5978\n",
            "Epoch 35, CIFAR-10 Batch 2:  Cost :0.61354, Accuracy on Training 0.9000 -Validation Accuracy: 0.5744\n",
            "Epoch 35, CIFAR-10 Batch 3:  Cost :0.58829, Accuracy on Training 0.8250 -Validation Accuracy: 0.5750\n",
            "Epoch 35, CIFAR-10 Batch 4:  Cost :0.63118, Accuracy on Training 0.8000 -Validation Accuracy: 0.5928\n",
            "Epoch 35, CIFAR-10 Batch 5:  Cost :0.68012, Accuracy on Training 0.8000 -Validation Accuracy: 0.5888\n",
            "Epoch 36, CIFAR-10 Batch 1:  Cost :0.66902, Accuracy on Training 0.7500 -Validation Accuracy: 0.6020\n",
            "Epoch 36, CIFAR-10 Batch 2:  Cost :0.60605, Accuracy on Training 0.8750 -Validation Accuracy: 0.5820\n",
            "Epoch 36, CIFAR-10 Batch 3:  Cost :0.58394, Accuracy on Training 0.8000 -Validation Accuracy: 0.5858\n",
            "Epoch 36, CIFAR-10 Batch 4:  Cost :0.62596, Accuracy on Training 0.7750 -Validation Accuracy: 0.6094\n",
            "Epoch 36, CIFAR-10 Batch 5:  Cost :0.67771, Accuracy on Training 0.7500 -Validation Accuracy: 0.5852\n",
            "Epoch 37, CIFAR-10 Batch 1:  Cost :0.67336, Accuracy on Training 0.8000 -Validation Accuracy: 0.6128\n",
            "Epoch 37, CIFAR-10 Batch 2:  Cost :0.53086, Accuracy on Training 0.9250 -Validation Accuracy: 0.5934\n",
            "Epoch 37, CIFAR-10 Batch 3:  Cost :0.65433, Accuracy on Training 0.8250 -Validation Accuracy: 0.5806\n",
            "Epoch 37, CIFAR-10 Batch 4:  Cost :0.62490, Accuracy on Training 0.8250 -Validation Accuracy: 0.6076\n",
            "Epoch 37, CIFAR-10 Batch 5:  Cost :0.66885, Accuracy on Training 0.8000 -Validation Accuracy: 0.6008\n",
            "Epoch 38, CIFAR-10 Batch 1:  Cost :0.66080, Accuracy on Training 0.8000 -Validation Accuracy: 0.6192\n",
            "Epoch 38, CIFAR-10 Batch 2:  Cost :0.60089, Accuracy on Training 0.8750 -Validation Accuracy: 0.5960\n",
            "Epoch 38, CIFAR-10 Batch 3:  Cost :0.61763, Accuracy on Training 0.8250 -Validation Accuracy: 0.5718\n",
            "Epoch 38, CIFAR-10 Batch 4:  Cost :0.61660, Accuracy on Training 0.8250 -Validation Accuracy: 0.6170\n",
            "Epoch 38, CIFAR-10 Batch 5:  Cost :0.65562, Accuracy on Training 0.7750 -Validation Accuracy: 0.6030\n",
            "Epoch 39, CIFAR-10 Batch 1:  Cost :0.64858, Accuracy on Training 0.8000 -Validation Accuracy: 0.6074\n",
            "Epoch 39, CIFAR-10 Batch 2:  Cost :0.55944, Accuracy on Training 0.9000 -Validation Accuracy: 0.5958\n",
            "Epoch 39, CIFAR-10 Batch 3:  Cost :0.57649, Accuracy on Training 0.8500 -Validation Accuracy: 0.5986\n",
            "Epoch 39, CIFAR-10 Batch 4:  Cost :0.65934, Accuracy on Training 0.7750 -Validation Accuracy: 0.6054\n",
            "Epoch 39, CIFAR-10 Batch 5:  Cost :0.63629, Accuracy on Training 0.8250 -Validation Accuracy: 0.6012\n",
            "Epoch 40, CIFAR-10 Batch 1:  Cost :0.66283, Accuracy on Training 0.8000 -Validation Accuracy: 0.6204\n",
            "Epoch 40, CIFAR-10 Batch 2:  Cost :0.54615, Accuracy on Training 0.8500 -Validation Accuracy: 0.5822\n",
            "Epoch 40, CIFAR-10 Batch 3:  Cost :0.56201, Accuracy on Training 0.8750 -Validation Accuracy: 0.6064\n",
            "Epoch 40, CIFAR-10 Batch 4:  Cost :0.56558, Accuracy on Training 0.8500 -Validation Accuracy: 0.5948\n",
            "Epoch 40, CIFAR-10 Batch 5:  Cost :0.61582, Accuracy on Training 0.8500 -Validation Accuracy: 0.6088\n",
            "Epoch 41, CIFAR-10 Batch 1:  Cost :0.69519, Accuracy on Training 0.7250 -Validation Accuracy: 0.6132\n",
            "Epoch 41, CIFAR-10 Batch 2:  Cost :0.55187, Accuracy on Training 0.8750 -Validation Accuracy: 0.5786\n",
            "Epoch 41, CIFAR-10 Batch 3:  Cost :0.57762, Accuracy on Training 0.8250 -Validation Accuracy: 0.5896\n",
            "Epoch 41, CIFAR-10 Batch 4:  Cost :0.57843, Accuracy on Training 0.8500 -Validation Accuracy: 0.6062\n",
            "Epoch 41, CIFAR-10 Batch 5:  Cost :0.59320, Accuracy on Training 0.8250 -Validation Accuracy: 0.6196\n",
            "Epoch 42, CIFAR-10 Batch 1:  Cost :0.64780, Accuracy on Training 0.7750 -Validation Accuracy: 0.6228\n",
            "Epoch 42, CIFAR-10 Batch 2:  Cost :0.50693, Accuracy on Training 0.8750 -Validation Accuracy: 0.5814\n",
            "Epoch 42, CIFAR-10 Batch 3:  Cost :0.55829, Accuracy on Training 0.8500 -Validation Accuracy: 0.6044\n",
            "Epoch 42, CIFAR-10 Batch 4:  Cost :0.53615, Accuracy on Training 0.9000 -Validation Accuracy: 0.6126\n",
            "Epoch 42, CIFAR-10 Batch 5:  Cost :0.60055, Accuracy on Training 0.8500 -Validation Accuracy: 0.6092\n",
            "Epoch 43, CIFAR-10 Batch 1:  Cost :0.61419, Accuracy on Training 0.8000 -Validation Accuracy: 0.6156\n",
            "Epoch 43, CIFAR-10 Batch 2:  Cost :0.54829, Accuracy on Training 0.8750 -Validation Accuracy: 0.5774\n",
            "Epoch 43, CIFAR-10 Batch 3:  Cost :0.54541, Accuracy on Training 0.8500 -Validation Accuracy: 0.6144\n",
            "Epoch 43, CIFAR-10 Batch 4:  Cost :0.55474, Accuracy on Training 0.8500 -Validation Accuracy: 0.6120\n",
            "Epoch 43, CIFAR-10 Batch 5:  Cost :0.58308, Accuracy on Training 0.8000 -Validation Accuracy: 0.6070\n",
            "Epoch 44, CIFAR-10 Batch 1:  Cost :0.61774, Accuracy on Training 0.8500 -Validation Accuracy: 0.6262\n",
            "Epoch 44, CIFAR-10 Batch 2:  Cost :0.53390, Accuracy on Training 0.8750 -Validation Accuracy: 0.5780\n",
            "Epoch 44, CIFAR-10 Batch 3:  Cost :0.50496, Accuracy on Training 0.8500 -Validation Accuracy: 0.6198\n",
            "Epoch 44, CIFAR-10 Batch 4:  Cost :0.52925, Accuracy on Training 0.8500 -Validation Accuracy: 0.6138\n",
            "Epoch 44, CIFAR-10 Batch 5:  Cost :0.53537, Accuracy on Training 0.8250 -Validation Accuracy: 0.6156\n",
            "Epoch 45, CIFAR-10 Batch 1:  Cost :0.60744, Accuracy on Training 0.8500 -Validation Accuracy: 0.6232\n",
            "Epoch 45, CIFAR-10 Batch 2:  Cost :0.46972, Accuracy on Training 0.8750 -Validation Accuracy: 0.6014\n",
            "Epoch 45, CIFAR-10 Batch 3:  Cost :0.50876, Accuracy on Training 0.8750 -Validation Accuracy: 0.6040\n",
            "Epoch 45, CIFAR-10 Batch 4:  Cost :0.53065, Accuracy on Training 0.8750 -Validation Accuracy: 0.6182\n",
            "Epoch 45, CIFAR-10 Batch 5:  Cost :0.54472, Accuracy on Training 0.8000 -Validation Accuracy: 0.6206\n",
            "Epoch 46, CIFAR-10 Batch 1:  Cost :0.59584, Accuracy on Training 0.8250 -Validation Accuracy: 0.6254\n",
            "Epoch 46, CIFAR-10 Batch 2:  Cost :0.48883, Accuracy on Training 0.8750 -Validation Accuracy: 0.5976\n",
            "Epoch 46, CIFAR-10 Batch 3:  Cost :0.51005, Accuracy on Training 0.8750 -Validation Accuracy: 0.6228\n",
            "Epoch 46, CIFAR-10 Batch 4:  Cost :0.50879, Accuracy on Training 0.8750 -Validation Accuracy: 0.6210\n",
            "Epoch 46, CIFAR-10 Batch 5:  Cost :0.56500, Accuracy on Training 0.8250 -Validation Accuracy: 0.6176\n",
            "Epoch 47, CIFAR-10 Batch 1:  Cost :0.56260, Accuracy on Training 0.8000 -Validation Accuracy: 0.6302\n",
            "Epoch 47, CIFAR-10 Batch 2:  Cost :0.48580, Accuracy on Training 0.9000 -Validation Accuracy: 0.5788\n",
            "Epoch 47, CIFAR-10 Batch 3:  Cost :0.50326, Accuracy on Training 0.9000 -Validation Accuracy: 0.6082\n",
            "Epoch 47, CIFAR-10 Batch 4:  Cost :0.59154, Accuracy on Training 0.7750 -Validation Accuracy: 0.5770\n",
            "Epoch 47, CIFAR-10 Batch 5:  Cost :0.61928, Accuracy on Training 0.8000 -Validation Accuracy: 0.6096\n",
            "Epoch 48, CIFAR-10 Batch 1:  Cost :0.57348, Accuracy on Training 0.8000 -Validation Accuracy: 0.6308\n",
            "Epoch 48, CIFAR-10 Batch 2:  Cost :0.46829, Accuracy on Training 0.8500 -Validation Accuracy: 0.5904\n",
            "Epoch 48, CIFAR-10 Batch 3:  Cost :0.43077, Accuracy on Training 0.9000 -Validation Accuracy: 0.6184\n",
            "Epoch 48, CIFAR-10 Batch 4:  Cost :0.50009, Accuracy on Training 0.9000 -Validation Accuracy: 0.6278\n",
            "Epoch 48, CIFAR-10 Batch 5:  Cost :0.54798, Accuracy on Training 0.8500 -Validation Accuracy: 0.6192\n",
            "Epoch 49, CIFAR-10 Batch 1:  Cost :0.58064, Accuracy on Training 0.8500 -Validation Accuracy: 0.6304\n",
            "Epoch 49, CIFAR-10 Batch 2:  Cost :0.45632, Accuracy on Training 0.8500 -Validation Accuracy: 0.5962\n",
            "Epoch 49, CIFAR-10 Batch 3:  Cost :0.45912, Accuracy on Training 0.9000 -Validation Accuracy: 0.6266\n",
            "Epoch 49, CIFAR-10 Batch 4:  Cost :0.52212, Accuracy on Training 0.8750 -Validation Accuracy: 0.5896\n",
            "Epoch 49, CIFAR-10 Batch 5:  Cost :0.55961, Accuracy on Training 0.8250 -Validation Accuracy: 0.6244\n",
            "Epoch 50, CIFAR-10 Batch 1:  Cost :0.61203, Accuracy on Training 0.7750 -Validation Accuracy: 0.6198\n",
            "Epoch 50, CIFAR-10 Batch 2:  Cost :0.44662, Accuracy on Training 0.8750 -Validation Accuracy: 0.6208\n",
            "Epoch 50, CIFAR-10 Batch 3:  Cost :0.44710, Accuracy on Training 0.9000 -Validation Accuracy: 0.6154\n",
            "Epoch 50, CIFAR-10 Batch 4:  Cost :0.50907, Accuracy on Training 0.8500 -Validation Accuracy: 0.6038\n",
            "Epoch 50, CIFAR-10 Batch 5:  Cost :0.52500, Accuracy on Training 0.8250 -Validation Accuracy: 0.6352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jx9yLpCGffgj"
      },
      "source": [
        "\n",
        "The model has been saved to disk.\n",
        "## Test Model\n",
        "Now needs to test the model against the test dataset.  This will be your final accuracy. The accuracy is greater than 50%. If the accuracy is not greater than 50%, the model architecture and parameters should be tweaked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WhYNuGaNffg1",
        "outputId": "1890c252-348c-4b03-b050-87df79f46ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Set batch size if not already set\n",
        "try:\n",
        "    if batch_size:\n",
        "        pass\n",
        "except NameError:\n",
        "    batch_size = 64\n",
        "\n",
        "save_model_path = './image_classification'\n",
        "n_samples = 4\n",
        "top_n_predictions = 3\n",
        "\n",
        "def test_model():\n",
        "    \"\"\"\n",
        "    Test the saved model against the test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
        "    loaded_graph = tf.Graph()\n",
        "\n",
        "    with tf.Session(graph=loaded_graph) as sess:\n",
        "        # Load model\n",
        "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "        loader.restore(sess, save_model_path)\n",
        "\n",
        "        # Get Tensors from loaded model\n",
        "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
        "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
        "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
        "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
        "        \n",
        "        # Get accuracy in batches for memory limitations\n",
        "        test_batch_acc_total = 0\n",
        "        test_batch_count = 0\n",
        "        \n",
        "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
        "            test_batch_acc_total += sess.run(\n",
        "                loaded_acc,\n",
        "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
        "            test_batch_count += 1\n",
        "\n",
        "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
        "\n",
        "        # Print Random Samples\n",
        "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
        "        random_test_predictions = sess.run(\n",
        "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
        "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
        "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
        "\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./image_classification\n",
            "Testing Accuracy: 0.6458984375\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP8z21GqV/u6fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 355,
              "height": 319
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eU1qedtbCsc",
        "colab_type": "text"
      },
      "source": [
        "By applying simple CNN model on a CIFAR-10 dataset having more than 50000 images, the accuracy is more than 60% which is quite good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YFasULIYffhE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}